services:
  llm:
    image: vllm/vllm-openai:latest
    container_name: llm
    restart: unless-stopped
    gpus: all
    ipc: host
    environment:
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    command:
      - vllm
      - serve
      - ${LLM_MODEL}
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --dtype
      - auto
      - --gpu-memory-utilization
      - ${VLLM_GPU_MEMORY_UTILIZATION}
      - --max-model-len
      - ${VLLM_MAX_MODEL_LEN}
      - --max-num-seqs
      - ${VLLM_MAX_NUM_SEQS}
      - --max-num-batched-tokens
      - ${VLLM_MAX_BATCHED_TOKENS}
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/v1/models').read()\""]
      interval: 15s
      timeout: 5s
      retries: 40

    # Optional: expose vLLM to the host for debugging
    # ports:
    #   - "127.0.0.1:8001:8000"

  api:
    build: .
    image: call-summary-api
    container_name: call-summary-api
    restart: unless-stopped
    depends_on:
      llm:
        condition: service_healthy
    environment:
      - LLM_BASE_URL=http://llm:8000/v1
      - LLM_MODEL=${LLM_MODEL}
      - MAX_INPUT_CHARS=${MAX_INPUT_CHARS}
      - CHUNK_CHARS=${CHUNK_CHARS}
      - REQUEST_TIMEOUT_SECS=${REQUEST_TIMEOUT_SECS}
    ports:
      - "8000:8000"
